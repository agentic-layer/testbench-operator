apiVersion: testworkflows.testkube.io/v1
kind: TestWorkflowTemplate
metadata:
  name: ragas-evaluate-template
  namespace: testkube
  labels:
    testkube.io/test-category: ragas-evaluation
    app: testworkflows
spec:
  # Configuration parameters that can be overridden
  config:
    model:
      type: string
      description: "Model name to use for evaluation (e.g., gemini-2.5-flash-lite)"
    openApiBasePath:
      type: string
      description: "Base path for OpenAI API"
      default: "http://ai-gateway-litellm.ai-gateway:4000"

  # Steps to execute
  steps:
    - name: evaluate-results
      artifacts:
        paths:
          - "data/results/evaluation_scores.json"
      run:
        command:
          - sh
          - -c
        args:
          - |
            uv run python3 evaluate.py "{{ config.model }}" --metrics-config "/app/config/metrics.yaml" && \
            if [ -f data/results/evaluation_scores.json ]; then
              echo "✓ Evaluation completed"
              cat data/results/evaluation_scores.json
            else
              echo "✗ Error: Results file not created"
              exit 1
            fi
        env:
          - name: OPENAI_API_BASE
            value: "{{ config.openApiBasePath }}"